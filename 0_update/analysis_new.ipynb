{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats, signal, optimize\n",
    "\n",
    "#potentiall remove at some point\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global parameters\n",
    "dt=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Find smallest $h$ for which the overlap with the zero-input case is less than discrimination error $\\varepsilon$\n",
    "Note: $a(0)=0$ for all $\\lambda$ and $\\mu$ such that $P(o)=\\mathcal{N}(0,\\sigma)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_overlap(pmf1, pmf2):\n",
    "    \"\"\"\n",
    "    calculates the overlap between two discrete probability mass functions\n",
    "    ATTENTION: needs user to ensure that domains are identical!\n",
    "    \"\"\"\n",
    "    assert len(pmf1) == len(pmf2)\n",
    "    return np.sum(np.minimum(pmf1, pmf2)) * 0.5\n",
    "\n",
    "\n",
    "def find_discriminable_inputs(pmf, h_range, epsilon: float, start=\"left\"):\n",
    "    \"\"\"\n",
    "    Determine all inputs h in range h_range such that the overlap between all pmfs is less than epsilon\n",
    "    The pmfs of h_range[0] and h_range[1] sever as boundaries\n",
    "\n",
    "    # Parameter\n",
    "    - pmf: function\n",
    "    - h_range: array-like with length two\n",
    "    - epsilon: float\n",
    "        discrimination error that specifies maximal overlap between two probability mass functions\n",
    "    \"\"\"\n",
    "    assert len(h_range) == 2\n",
    "    h_left, h_right = h_range\n",
    "    hs = []\n",
    "    if start == \"left\":\n",
    "        h_ref = h_left\n",
    "        pmf_end = pmf(h_right)\n",
    "    elif start == \"right\":\n",
    "        h_ref = h_right\n",
    "        pmf_end = pmf(h_left)\n",
    "    pmf_ref = pmf(h_ref)\n",
    "\n",
    "    while True:\n",
    "        def func(h):\n",
    "            return calc_overlap(pmf_ref, pmf(h)) - epsilon\n",
    "\n",
    "        try:\n",
    "            if start == \"left\":\n",
    "                h_new = optimize.bisect(func, h_ref, h_right)\n",
    "            elif start == \"right\":\n",
    "                h_new = optimize.bisect(func, h_left, h_ref)\n",
    "            pmf_new = pmf(h_new)\n",
    "            if calc_overlap(pmf_end, pmf_new) < epsilon:\n",
    "                hs.append(h_new)\n",
    "                h_ref = h_new\n",
    "                pmf_ref = pmf_new\n",
    "            else:\n",
    "                break\n",
    "        except:\n",
    "            break\n",
    "    return hs\n",
    "\n",
    "\n",
    "def dynamic_range(h_range):\n",
    "    \"\"\"\n",
    "    Calculate the dynamic range from the range h_range\n",
    "    \"\"\"\n",
    "    assert len(h_range) == 2\n",
    "    h_left, h_right = h_range\n",
    "    return 10 * (np.log10(h_right) - np.log10(h_left))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test discriminable intervals\n",
    "N = int(1e4)\n",
    "mu = 0.2\n",
    "lam = 0.99999\n",
    "sigma=1e-2\n",
    "Xs = np.arange(-4*sigma*N,N+4*sigma*N)\n",
    "epsilon=0.02\n",
    "\n",
    "def pmf(h):\n",
    "    # see below for formal derivation, here just as a test\n",
    "    A = mu*(1-np.exp(-h*dt))/(1-lam*(1-mu)-lam*mu*np.exp(-h*dt)) * N\n",
    "    # in domain with delta = 1\n",
    "    delta = 1\n",
    "    return stats.norm.pdf(Xs, A, sigma*N)*delta\n",
    "\n",
    "h_range=[0,1e3]\n",
    "pmf_ref_left = pmf(h_range[0])\n",
    "pmf_ref_right = pmf(h_range[1])\n",
    "\n",
    "plt.plot(Xs,pmf_ref_left, color=\"gray\")\n",
    "plt.plot(Xs,pmf_ref_right, color=\"gray\")\n",
    "\n",
    "hs_left = find_discriminable_inputs(pmf,h_range, epsilon, start=\"left\")\n",
    "print(hs_left)\n",
    "for h in hs_left:\n",
    "    plt.plot(Xs,pmf(h), color=\"blue\")\n",
    "\n",
    "hs_right = find_discriminable_inputs(pmf,h_range, epsilon, start=\"right\")\n",
    "print(hs_right)\n",
    "for h in hs_right:\n",
    "    plt.plot(Xs,pmf(h), color=\"orange\")\n",
    "#plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consider network output with Gaussian noise\n",
    "The output of the network is the sum of the network activity and Gaussian noise $\\eta\\sim\\mathcal{N}(0,1)$ such that \n",
    "$$ o = a + \\sigma\\eta $$\n",
    "The output is thus a convolution between $P(a)$ and $\\mathcal{N}(0,\\sigma)$.\n",
    "\n",
    "For $T\\to\\infty$ the distribution of activity is a delta-distribution such that the the output distribution becomes a shifted Gaussian\n",
    "$$ P(o) = \\mathcal{N}(a(h|\\lambda,\\mu), \\sigma)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analytic solution for $T\\to\\infty$\n",
    "For the $T\\to\\infty$ case, we have a one-to-one mapping between input $h$ and network activity $a$, given by \n",
    "$$a(h|\\lambda, \\mu) = \\frac{\\mu\\left(1-e^{-h\\Delta t}\\right)}{1-\\lambda(1-\\mu)-\\lambda\\mu e^{-h\\Delta t}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a(h,lam,mu):\n",
    "    return mu*(1-np.exp(-h*dt))/(1-lam*(1-mu)-lam*mu*np.exp(-h*dt))\n",
    "\n",
    "hs=np.logspace(-5,1,20)\n",
    "colors=['red','blue','orange','green']\n",
    "for i,lam in enumerate([0,0.9,0.99,0.999]):\n",
    "    plt.plot(hs*0.2, a(hs, lam, 0.2), label='{}'.format(lam), color=colors[i], alpha=0.5)\n",
    "    plt.plot(hs, a(hs, lam, 1.0), color=colors[i])\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"input\")\n",
    "plt.ylabel(\"activity\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network parameters\n",
    "N = int(1e4)\n",
    "mu = 0.2\n",
    "sigma = 1e-2\n",
    "Xs = np.arange(-4 * sigma * N, N + 4 * sigma * N)\n",
    "epsilon = 0.2\n",
    "lams = 1 - np.logspace(-4, 0, 30)\n",
    "\n",
    "# analysis parameters\n",
    "drs = np.zeros(len(lams))\n",
    "nds = np.zeros(len(lams))\n",
    "mis = np.zeros(len(lams))\n",
    "h_range = [0, 1e3]\n",
    "# attempt to match previous data analysis\n",
    "logh = np.arange(-7, 1.5, 0.05)  # 170 elements\n",
    "for i, lam in tqdm(enumerate(lams), total=len(lams)):\n",
    "    # distribution of noisy output\n",
    "    def pmf_o_given_h(h):\n",
    "        A = a(h, lam, mu) * N\n",
    "        return stats.norm.pdf(Xs, A, sigma * N)\n",
    "\n",
    "    hs_left = find_discriminable_inputs(pmf_o_given_h, h_range, epsilon)\n",
    "    hs_right = find_discriminable_inputs(pmf_o_given_h, h_range, epsilon, start=\"right\")\n",
    "    drs[i] = dynamic_range((hs_left[0], hs_right[0]))\n",
    "    nds[i] = 0.5 * (len(hs_left) + len(hs_right))\n",
    "\n",
    "# plot the number of discriminable inputs for different values of epsilon and T in a main plot with an inset\n",
    "markers = [\"o\", \"s\", \"^\", \"P\", \"d\"]\n",
    "legends = [\"$1$ ms\", \"$10$ ms\", \"$10^2$ ms\", \"$10^3$ ms\", \"$10^4$ ms\"]\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "ax[0].plot(1 - lams, nds)\n",
    "ax[0].set_xlabel(\"branching parameter $\\lambda$\")\n",
    "ax[0].set_ylim(0, 65)\n",
    "ax[0].set_ylabel(\"number of discriminable inputs $n_d$\")\n",
    "ax[0].set_xscale(\"log\")\n",
    "ax[0].set_xticks([1e-4, 1e-3, 1e-2, 1e-1, 1e0])\n",
    "ax[0].set_xticklabels([\"0.9999\", \"0.999\", \"0.99\", \"0.9\", \"0\"])\n",
    "ax[0].invert_xaxis()\n",
    "ax[1].plot(1 - lams, drs)\n",
    "ax[1].set_xlabel(\"branching parameter $\\lambda$\")\n",
    "ax[1].set_ylim(0, 40)\n",
    "ax[1].set_ylabel(\"dynamic range $\\Delta$\")\n",
    "ax[1].set_xscale(\"log\")\n",
    "ax[1].set_xticks([1e-4, 1e-3, 1e-2, 1e-1, 1e0])\n",
    "ax[1].set_xticklabels([\"0.9999\", \"0.999\", \"0.99\", \"0.9\", \"0\"])\n",
    "ax[1].invert_xaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load parameters fo beta distribution from file\n",
    "def load_beta_params(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        beta_params = pickle.load(f)\n",
    "    return beta_params\n",
    "\n",
    "beta_params = load_beta_params('../data/betaParams/subpop_k=100/epsilon=1.00e-01_subFix_window=1_realization=0.pkl')\n",
    "print(beta_params.keys())\n",
    "\n",
    "# plot beta parameters for different values of epsilon in two parallel subplots (a and b)\n",
    "fig, axes = plt.subplots(2, 1, figsize=(10, 5))\n",
    "epsilons = [1e-1, 1e-2, 1e-3, 1e-4]\n",
    "epsilons = np.logspace(-4, 0, 9).tolist()\n",
    "for epsilon in epsilons:\n",
    "    beta_params = load_beta_params('../data/betaParams/subpop_k=100/epsilon={:.2e}_subFix_window=1_realization=0.pkl'.format(epsilon))\n",
    "    # filter out invalid values (a,b < 0)\n",
    "    mask = (beta_params['a'] > 0) & (beta_params['b'] > 0)\n",
    "    loghs = beta_params['logh'][mask]\n",
    "    valas = beta_params['a'][mask]\n",
    "    valbs = beta_params['b'][mask]\n",
    "    #plot\n",
    "    axes[0].scatter(loghs, valas, label='epsilon={}'.format(epsilon))    \n",
    "    axes[1].scatter(loghs, valbs, label='epsilon={}'.format(epsilon))    \n",
    "\n",
    "axes[0].set_xlabel(\"log h\")\n",
    "axes[0].set_ylabel(\"a\")\n",
    "axes[1].set_xlabel(\"log h\")\n",
    "axes[1].set_ylabel(\"b\")\n",
    "axes[0].set_yscale(\"log\")\n",
    "axes[1].set_yscale(\"log\")\n",
    "#axes[1].legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the available data with a deep neural network that maps system parameters (h,epsilon) to beta-distribution parameters (a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# load data\n",
    "x_data = []\n",
    "y_data = []\n",
    "\n",
    "def get_data(realization, epsilons=np.logspace(-4, 0, 9).tolist(), shuffle=1234):\n",
    "    X1, X2 = np.empty(shape=(0,)), np.empty(shape=(0,))\n",
    "    Y1, Y2 = np.empty(shape=(0,)), np.empty(shape=(0,))\n",
    "    for epsilon in epsilons:\n",
    "        beta_params = load_beta_params('../data/betaParams/subpop_k=100/epsilon={:.2e}_subFix_window=1_realization={:d}.pkl'.format(epsilon, realization))\n",
    "        # filter out invalid values where either a or b are negative\n",
    "        mask = (beta_params['a'] > 0) & (beta_params['b'] > 0)\n",
    "        loghs = beta_params['logh'][mask]\n",
    "        loges = np.log(epsilon) * np.ones_like(loghs)\n",
    "        logas = np.log(beta_params['a'][mask])\n",
    "        logbs = np.log(beta_params['b'][mask])\n",
    "        X1 = np.append(X1, loghs)\n",
    "        X2 = np.append(X2, loges)\n",
    "        Y1 = np.append(Y1, logas)\n",
    "        Y2 = np.append(Y2, logbs)\n",
    "    # shuffle with random mask\n",
    "    mask = np.arange(len(X1))\n",
    "    # if shuffle is not False:\n",
    "    if shuffle is not None:\n",
    "        np.random.seed(shuffle)\n",
    "        np.random.shuffle(mask)\n",
    "    x_data = np.stack((X1[mask], X2[mask]), axis=1)\n",
    "    y_data = np.stack((Y1[mask], Y2[mask]), axis=1)\n",
    "    print(realization, x_data.shape, y_data.shape)\n",
    "    return x_data, y_data, \n",
    "get_data(1, shuffle=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We follow here the [simple example](https://notebook.community/kit-cel/lecture-examples/mloc/ch3_Deep_Learning/pytorch/function_approximation_with_MLP) using pytorch\n",
    "\n",
    "Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden1 = nn.Linear(2, 42)\n",
    "        self.act1 = nn.Tanh()\n",
    "        self.hidden2 = nn.Linear(42, 42)\n",
    "        self.act2 = nn.Tanh()\n",
    "        self.hidden3 = nn.Linear(42, 42)\n",
    "        self.act3 = nn.Tanh()\n",
    "        self.output = nn.Linear(42, 2)\n",
    "        #self.act_output = nn.Sigmoid()\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.hidden1(x))\n",
    "        x = self.act2(self.hidden2(x))\n",
    "        x = self.act3(self.hidden3(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    " \n",
    "\n",
    "def train_model_simple(X_train, Y_train, epochs):\n",
    "    \"\"\"\n",
    "    Trains a simple neural network with one hidden layer with units neurons\n",
    "    \"\"\"\n",
    "    # prepare data\n",
    "    X_train_tensor = torch.from_numpy(X_train).float()\n",
    "    Y_train_tensor = torch.from_numpy(Y_train).float()\n",
    "\n",
    "    # create model\n",
    "    model = NeuralNetwork()\n",
    "\n",
    "    # Adam and MSE Loss\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    loss_fn = nn.MSELoss(reduction='mean')\n",
    "    \n",
    "    # main loop    \n",
    "    history_loss = []\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        yhat = model(X_train_tensor)\n",
    "        loss = loss_fn(yhat, Y_train_tensor)\n",
    "        history_loss.append(loss.item())\n",
    "        # compute gradients\n",
    "        loss.backward() \n",
    "        # carry out one optimization step with Adam\n",
    "        optimizer.step()   \n",
    "        # reset gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    return model, history_loss\n",
    "\n",
    "# rescale data (can use the same scaler multiple times using fit_transform function)\n",
    "X_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "Y_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "X, Y = get_data(0)\n",
    "X_scaled = X_scaler.fit_transform(X)\n",
    "Y_scaled = Y_scaler.fit_transform(Y)\n",
    "\n",
    "# train model\n",
    "model, loss = train_model_simple(X_scaled, Y_scaled, 10000)\n",
    "plt.plot(loss)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.yscale('log')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "#compare to data\n",
    "X, Y = get_data(0, shuffle=None)\n",
    "# mesh grid for contour plot\n",
    "loghs = np.linspace(-7, 1.5, 170)\n",
    "loges = np.linspace(-4, 0, 9)\n",
    "X_mesh = np.meshgrid(loghs, loges)\n",
    "#print(X_mesh.shape)\n",
    "\n",
    "# compare to data\n",
    "fig, axes = plt.subplots(2, 1, figsize=(10, 5))\n",
    "X,Y = get_data(0, shuffle=None)\n",
    "# plot for all epsilon (always same color!)\n",
    "\n",
    "for epsilon in np.logspace(-4, 0, 9).tolist()[:-1]:\n",
    "    # plot data\n",
    "    mask = X[:,1]==np.log(epsilon)\n",
    "    ref = axes[0].scatter(X[mask,0], np.exp(Y[mask, 0]), label='epsilon={}'.format(epsilon))\n",
    "    color = ref.get_facecolor()[0]\n",
    "    axes[1].scatter(X[mask,0], np.exp(Y[mask, 1]), label='epsilon={}'.format(epsilon), color=color)\n",
    "    # plot model\n",
    "    loghs = np.linspace(-7, 1.5, 200)\n",
    "    loges = np.log(epsilon) * np.ones_like(loghs)\n",
    "    X_model = np.stack((loghs, loges), axis=1)\n",
    "    X_model_scaled = X_scaler.transform(X_model)\n",
    "    Y_model_scaled = model(torch.from_numpy(X_model_scaled).float()).detach().numpy()\n",
    "    Y_model = Y_scaler.inverse_transform(Y_model_scaled)\n",
    "    axes[0].plot(loghs, np.exp(Y_model[:, 0]), color=color)\n",
    "    axes[1].plot(loghs, np.exp(Y_model[:, 1]), color=color)\n",
    "\n",
    "axes[0].set_xlabel(\"log h\")\n",
    "axes[0].set_ylabel(\"a\")\n",
    "axes[1].set_xlabel(\"log h\")\n",
    "axes[1].set_ylabel(\"b\")\n",
    "axes[0].set_yscale(\"log\")\n",
    "axes[1].set_yscale(\"log\")\n",
    "#axes[1].legend()\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the interpolation model we can obtain estimates of the dynamic range and the resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1e-1\n",
    "b = 100\n",
    "delta = 1/N\n",
    "support = np.arange(-1-4*sigma, 1 + 4*sigma, delta)\n",
    "pmf_beta = stats.beta.pdf(support, a, b)*delta\n",
    "pmf_norm = stats.norm.pdf(support, 0, sigma)*delta\n",
    "print(np.sum(pmf_beta), np.sum(pmf_norm))\n",
    "print(pmf_beta)\n",
    "# convolution with a Gaussian distribution at every point of the domain\n",
    "conv = signal.fftconvolve(pmf_beta, pmf_norm, mode='same')\n",
    "print(np.sum(conv))\n",
    "mask = conv > 1e-5\n",
    "plt.plot(support[mask], pmf_beta[mask])\n",
    "plt.plot(support[mask], pmf_norm[mask])\n",
    "plt.plot(support[mask], conv[mask])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test discriminable intervals (TODO: here is sth still not quite as it is supposed to be ... overlap between distributions is not epsilon) ... am I missing the convolution!!\n",
    "N = int(1e4)\n",
    "mu = 0.2\n",
    "lam = 0.999\n",
    "sigma=1e-2\n",
    "# need delta 1/N to be consistent with the overlap error that is defined on the level of the pmf with activity unit of neurons?\n",
    "delta = 1/N\n",
    "domain = np.arange(-4*sigma - 1, 1 + 4*sigma, delta)\n",
    "epsilon=0.02\n",
    "\n",
    "def pmf(h, convolve=True):\n",
    "    # distribution is given by Beta-distribution specified by a and b\n",
    "    X = np.log(np.array([h,epsilon]))\n",
    "    X_scaled = X_scaler.transform(X.reshape(-1,1).T)\n",
    "    Y_scaled = model(torch.from_numpy(X_scaled).float()).detach().numpy()\n",
    "    loga,logb = Y_scaler.inverse_transform(Y_scaled).T.reshape(-1)\n",
    "    a,b = np.exp(loga), np.exp(logb)\n",
    "    if convolve:\n",
    "        pmf_beta = stats.beta.pdf(domain, a, b)*delta\n",
    "        pmf_norm = stats.norm.pdf(domain, 0, sigma)*delta\n",
    "        # convolution with a Gaussian distribution at every point of the domain\n",
    "        return np.convolve(pmf_beta, pmf_norm, mode=\"same\")\n",
    "    else:\n",
    "        return stats.beta.pdf(domain, a, b)*delta\n",
    "\n",
    "h_range=[1e-7,1e3]\n",
    "pdf_ref_left = stats.norm.pdf(domain, 0, sigma)*delta\n",
    "pdf_ref_right = stats.norm.pdf(domain, 1, sigma)*delta\n",
    "\n",
    "hs_left = find_discriminable_inputs(pmf, h_range, epsilon, start=\"left\")\n",
    "print(\"hs_left: \", hs_left)\n",
    "for h in hs_left:\n",
    "    plt.plot(domain,pmf(h), color=\"blue\")\n",
    "\n",
    "hs_right = find_discriminable_inputs(pmf, h_range, epsilon, start=\"right\")\n",
    "print(\"hs_right: \", hs_right)\n",
    "for h in hs_right:\n",
    "    plt.plot(domain,pmf(h), color=\"orange\")\n",
    "\n",
    "plt.plot(domain,pdf_ref_left, color=\"gray\")\n",
    "plt.plot(domain,pdf_ref_right, color=\"gray\")\n",
    "plt.xlim(-4*sigma,1)\n",
    "\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilons = np.logspace(-4, 0, 30)\n",
    "\n",
    "# analysis parameters\n",
    "drs = np.zeros(len(epsilons))\n",
    "nds = np.zeros(len(epsilons))\n",
    "mis = np.zeros(len(epsilons))\n",
    "# need to exclude the zero here because of logh fit\n",
    "h_range = [1e-7, 1e1]\n",
    "for i, epsilon in enumerate(epsilons):\n",
    "    print(i,epsilon)\n",
    "    # distribution is given by Beta-distribution specified by a and b\n",
    "    def pmf_o_given_h(h):\n",
    "        X = np.log(np.array([h,epsilon]))\n",
    "        X_scaled = X_scaler.transform(X.reshape(-1,1).T)\n",
    "        Y_scaled = model(torch.from_numpy(X_scaled).float()).detach().numpy()\n",
    "        loga,logb = Y_scaler.inverse_transform(Y_scaled).T.reshape(-1)\n",
    "        a,b = np.exp(loga), np.exp(logb)\n",
    "        return stats.beta.pdf(Xs/N, a, b)\n",
    "        \n",
    "    hs_left = find_discriminable_inputs(pmf_o_given_h, h_range, epsilon)\n",
    "    hs_right = find_discriminable_inputs(pmf_o_given_h, h_range, epsilon, start=\"right\")\n",
    "    print(hs_left,hs_right)\n",
    "    drs[i] = dynamic_range((hs_left[0], hs_right[0]))\n",
    "    nds[i] = 0.5 * (len(hs_left) + len(hs_right))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "- Develop an approximation to the finite-time solution as a correction to the infinite T limit?\n",
    "- Fit this?\n",
    "- Or take absolute maximum? -> as inset?\n",
    "- Only Fig n_d and DR ... Lets try to replot with development of maxima as inset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scipy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
